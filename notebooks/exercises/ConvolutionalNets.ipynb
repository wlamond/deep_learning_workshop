{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Flatten, Input\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.models import Model, load_model\n",
    "from keras.utils import to_categorical\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "IMG_HEIGHT, IMG_WIDTH = 28, 28\n",
    "N_CLASSES = 10\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 5\n",
    "TRAINED_WEIGHTS_FILENAME = 'model_weights.hdf5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Keras\n",
    "\n",
    "Keras is a library for defining and training neural networks. It lets you define network architectures declaritively, and gives you an API similar to scikit-learn for training neural networks.\n",
    "\n",
    "First, let's see an example of a densly connected neural net trained on MNIST digits.\n",
    "\n",
    "First, we'll prepare the data. Dense layers in Keras expect the input to be a vector, so we can't directly use the 2D matrix representation of the MNIST digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_flattened_data():\n",
    "    \n",
    "    (X_train, y_train), (X_val, y_val) = mnist.load_data()\n",
    "    \n",
    "    def _prep_data(X, y):\n",
    "        # flatten the data for our dense net\n",
    "        X = X.reshape(-1, IMG_HEIGHT * IMG_WIDTH)\n",
    "        # Each pixel is represented by an 8 bit unsigned integer. \n",
    "        # Keras expects all inputs to be 32 bit floats\n",
    "        X = X.astype(np.float32)\n",
    "        # Squash pixel values to be between 0 and 1. \n",
    "        # This tends to help improve learning, but isn't strictly necessary.\n",
    "        X /= 255\n",
    "        y = to_categorical(y)\n",
    "        return X, y\n",
    "    \n",
    "    X_train, y_train = _prep_data(X_train, y_train)\n",
    "    X_val, y_val = _prep_data(X_val, y_val)\n",
    "    return (X_train, y_train), (X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_val, y_val) = prepare_flattened_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define a densly connected neural network with two hidden layers to get an idea of how models are defined in Keras. \n",
    "\n",
    "![title](../img/neural_net2.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_dense_model():\n",
    "    \"\"\"Set up a dense model.\n",
    "    \n",
    "    Note that this function doesn't do any actual model computation. It only \n",
    "    declares the structure of the neural network, which defines what kinds \n",
    "    of operations will be done by the model during inference.\"\"\"\n",
    "    \n",
    "    # An input placeholder. This tells Keras the shape of the model input.\n",
    "    flat_image = Input(shape=(IMG_HEIGHT * IMG_WIDTH,))\n",
    "    \n",
    "    # Defnies a dense layer with 512 units and a sigmoid activation function.\n",
    "    # Note that layers are callable, and this layer takes the flat image placehodler as input.\n",
    "    # Since it isn't the input \"layer\" or output layer, it is considered a \"hidden layer\".\n",
    "    dense_1 = Dense(512, activation='sigmoid')(flat_image)\n",
    "    \n",
    "    # Another dense layer. This time, the input is the previous dense layer.\n",
    "    dense_2 = Dense(512, activation='sigmoid')(dense_1)\n",
    "    \n",
    "    # A final dense layer that has as many units as there are classes we want to recognize.\n",
    "    # The activation this time is a softmax function, which will convert the output to \n",
    "    # something that can be interpreted as a probability distribution.\n",
    "    prediction = Dense(N_CLASSES, activation='softmax')(dense_2)\n",
    "    \n",
    "    # A Model is a directed acyclic graph from inputs to outputs. A Model object has an\n",
    "    # API similar to scikit learn with `fit`, `predict`, and `evaluate` methods.\n",
    "    model = Model(inputs=flat_image, outputs=prediction)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = setup_dense_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: When compiling, include accuracy as a metric\n",
    "\n",
    "Refer to [Keras docs on metrics](https://keras.io/metrics/) to learn more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models must be compiled. This step tells the model what objective function to minimize, and what \n",
    "# optimization algorithm to use. There are a ton of options available out of the box for objectives\n",
    "# and optimizers, and you may also define your own.\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# The model is trained by sampling batches of examples and labels and peforming SGD.\n",
    "# Each pass over the training data is referred to as an \"epoch\" and multiple epochs are \n",
    "# typically required for the model to converge.\n",
    "train_history = model.fit(X_train, y_train,\n",
    "                          batch_size=BATCH_SIZE, \n",
    "                          epochs=EPOCHS, \n",
    "                          validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One way of better understanding our model is by observing the learning curves generated from the training and validation\n",
    "# losses over time. The shape of these curves can tell is if the model is suffering from a bias or variance problem and \n",
    "# can provide clues for what to do next to improve performance.\n",
    "\n",
    "def plot_train_history(history):\n",
    "    train_loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    epoch_labels = range(1, EPOCHS + 1)\n",
    "    print epoch_labels\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.set_xticks(epoch_labels)\n",
    "    ax.plot(epoch_labels, train_loss, label=\"Training\")\n",
    "    ax.plot(epoch_labels, val_loss, label=\"Validation\")\n",
    "    ax.xaxis.set_major_formatter(matplotlib.ticker.FormatStrFormatter('%d'))\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_train_history(train_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Representation Learning\n",
    "\n",
    "We didn't need to make many assumptions about our data for the dense model, and this tends to be true for neural networks in general. By using the hidden layers, we let the model learn its own representation of the input data that it then used to make the final prediction. This notion of representation learning is key to neural network performance. The network needs to identify meaningful information from the raw input in order to learn a suitable representation for each of the hidden layers, and these representations must contain suitable information for the final layer to make a correct prediction. \n",
    "\n",
    "\n",
    "# Convolutional Nets\n",
    "\n",
    "One assumption that we can make to improve performance for vision tasks is that neighboring pixels are correlated, and that correlation is meaningful. This is the primary assumption that underlies convolutional neural networks. Let's dig in to the operations that are used within convnets.\n",
    "\n",
    "\n",
    "## Convolutions\n",
    "\n",
    "Convolutions are the workhorse of deep nets for computer vision. Each convolutional layers consist of a set of kernels (often called filters) that slide, or \"convolve<sup>1</sup>\", over the layer's input. At each position, the dot product between the kernel and the section of input is computed and becomes the output in that spatial position. The output of the dot product is maximal when the input best matches the kernel, signaling to the next layer that the pattern detected by the kernel is present at that location in the input. This exploits the fact that a pattern recognized anywhere in the image may still be relevant, regardless of where in the image it is. It also lets us do this pattern recognition with far fewer parameters than we would need if we used a fully connected neural network.\n",
    "\n",
    "<sup>1</sup> In practice, most libraries actually implement a cross correlation rather than a true convolution.\n",
    "\n",
    "![title](../img/convolution.gif)\n",
    "\n",
    "There are typically tens or hundreds of kernels in each convolutional layer, leading to tens or hundreds of patterns that can be recognized by the layer anywhere in the image. These patterns tend to form a hierarchy. The layers closest to the input detect simple patterns such as edges and splashes of color. These low-level patterns are composed into more abstract patterns as signal moves through the network. By the end, the kernels are able to recognize abstract patterns that are useful for discriminating between classes. This representation can be understood as the features of the image, and then used in shallow neural nets (like in our example above) or in a linear model.\n",
    "\n",
    "![title](../img/convnet_filters.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pooling\n",
    "\n",
    "In a pooling layer, a patch of input is combined into a single output by taking the maximum value or averaging the values. The patches may have some overlap, or they may be completely distinct from one another.\n",
    "\n",
    "![title](../img/max_pooling.png)\n",
    "\n",
    "Pooling accomplishes two things. First, it contributes to learning translation invariance by combining information in adjacent areas of the input. Small translations in the input will be ignored by the pooling layer, and thus all subsequent layers. Second, it reduces the size of the intermediate representation, which reduces the memory footprint of all intermediate representations throughout the rest of the network by reducing the number of activations. This also helps combat overfitting.\n",
    "\n",
    "Recent work from Geoff Hinton seeks to eliminate pooling by using a new kind of layer called a \"capsule layer\". This has demonstrated great performance on small tasks such as MNIST classification, but hasn't quite yet usurped pooling in practical applications. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout\n",
    "\n",
    "Dropout is a form of regularization that tends to work quite well in deep nets. Dropout layers randomly remove neurons in a layer, changing the internal activation structure of the network. Intuitively, this forces each neuron to contibute to the final prediction rather than relying on other neurons to do a good job. In theory, a network trained with dropout behaves like an ensemble of neural networks. \n",
    "\n",
    "![title](../img/dropout.png)\n",
    "\n",
    "At inference time, the expected output of a neuron is computed using the actual output and the probability of keeping the neuron. This keeps the output of the network the same for each input. \n",
    "\n",
    "There's some fascinating new work that shows how sampling from a network trained with dropout lets you compute the confidence of the network's predictions. This provides a new way to understand how the network is behaving, and gives you more information to be used when making decisions about the model's output in downstream applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making it all work\n",
    "\n",
    "Let's train a convnet on the same task as the dense model. \n",
    "\n",
    "First, we'll load and prepare the data for this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_2d_data():\n",
    "    (X_train, y_train), (X_val, y_val) = mnist.load_data()\n",
    "\n",
    "    def _prepare_data(X, y):        \n",
    "        X = np.expand_dims(X, axis=3)  # add a color channel to each image\n",
    "        X = X.astype(np.float32)  # convert from 8 bit ints to floats\n",
    "        X /= 255  # squash pixel values to be between 0 and 1\n",
    "        y = to_categorical(y, N_CLASSES)\n",
    "        return X, y\n",
    "    \n",
    "    X_train, y_train = _prepare_data(X_train, y_train)\n",
    "    X_val, y_val = _prepare_data(X_val, y_val)\n",
    "    return (X_train, y_train), (X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_val, y_val) = prepare_2d_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now visualize our examples since we're training on actual 2D images (rather than flattened images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_digit(img):\n",
    "    \"\"\"Undo all preprocessing to a single image and display it\"\"\"\n",
    "    img = (img * 255).astype(np.uint8)  # unsquash and convert back to 8 bit ints\n",
    "    img = np.squeeze(img, axis=2)  # remove color channel data to render grayscale image\n",
    "    return Image.fromarray(img, mode='L')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unsquash one image and view it\n",
    "digit_idx = np.random.choice(np.arange(len(X_train)))\n",
    "print 'label:', np.argmax(y_train[digit_idx])\n",
    "display_digit(X_train[digit_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Follow the specification in this function's comments to create a convnet\n",
    "\n",
    "Refer to the [Keras docs](https://keras.io/) to learn more about the different layer types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_convnet():\n",
    "    \"\"\"Set up a convolutional net for digit classification\"\"\"\n",
    "    img = Input(shape=(IMG_HEIGHT, IMG_WIDTH, 1))\n",
    "    # conv layer with 32 filters, size 3x3, relu activation\n",
    "    # conv layer with 64 filters, size 3x3, relu activation\n",
    "    # max pooling 2d layer with pool size 2x2 and stride 2x2\n",
    "    # dropout with probability 0.25\n",
    "    # flatten the representation to be used by dense layers\n",
    "    # dense layer with 128 units, relu activation\n",
    "    # dropout with probability 0.25    \n",
    "    digit = # ... dense layer with 10 units, softmax activation\n",
    "    model = Model(inputs=img, outputs=digit)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = setup_convnet()\n",
    "model.compile(loss=categorical_crossentropy, optimizer='adam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the layer with the most parameters is the dense layer right after the flatten layer. Convolution layers tend to be very efficient in the number of parameters. Most modern convnets are \"fully convolutional\", opting to use convolutions for all representation learning and only using a single dense layer to structure the predictions at the end of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Using Keras callbacks, save a checkpoint of your model after every epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "train_history = model.fit(X_train, y_train,\n",
    "                          batch_size=BATCH_SIZE, \n",
    "                          epochs=EPOCHS, \n",
    "                          callbacks=[], # include callbacks\n",
    "                          validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_train_history(train_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample an image + label to check the model qualitatively\n",
    "sampled_id = np.random.choice(np.arange(len(X_val)))\n",
    "img, label = X_val[sampled_id], y_val[sampled_id]\n",
    "prediction = model.predict(np.expand_dims(img, axis=0))  # keras always expects a batch dimension, so we must expand\n",
    "print 'label:', np.argmax(label), 'prediction:', np.argmax(prediction)\n",
    "display_digit(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model for later use\n",
    "model.save(TRAINED_WEIGHTS_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model later on to use it\n",
    "model = load_model(TRAINED_WEIGHTS_FILENAME)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
